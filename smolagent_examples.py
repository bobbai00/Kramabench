# -*- coding: utf-8 -*-
"""quickstart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pXi5ffBFNJQ5nn1111SnIfjfKCOlunxu

# DABstep Benchmark Qwen2.5-Coder-32B Baseline

This notebook will guide you though submitting a Qwen2.5-Coder-32B baseline to the DABstep leaderboard.

* Live ðŸ¤— Leaderboard: https://huggingface.co/spaces/adyen/DABstep
* Benchmark ðŸ¤— Dataset: https://huggingface.co/datasets/adyen/DABstep
* LLM Agent Framework by ðŸ¤—: https://github.com/huggingface/smolagents/tree/main

## Environment Setup

We need to setup:
* **HuggingFace Token:** In order to make free API calls to HuggingFace Inference API you must have a HF account, the API verifies this by checking your account's token. This token will not be used for anything else.
* **Benchmark context files:** In order to solve the benchmark tasks the agent will need to reference documentation and analyze data which is spread out across multiple files, just like a real Data Analyst would.

### HuggingFace Token Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install smolagents==1.6.0 datasets==3.2.0
# we install also the evaluation logic from the DABstep leaderboard, so we can locally evaluate without having to make a submission
# %pip install git+https://git@hf.co/spaces/adyen/DABstep.git@main

import time
import os
import json
import datasets
import pandas as pd
from smolagents import CodeAgent
from smolagents.agents import ActionStep
from smolagents.models import HfApiModel
from huggingface_hub import login, hf_hub_download
from dabstep_benchmark.utils import evaluate


login(new_session=True)

"""#### Download context files
First we download the context files from the [Benchmark's Dataset](https://huggingface.co/datasets/adyen/DABstep) so that our agent can access them.

"""

CONTEXT_FILENAMES = [
    "data/context/acquirer_countries.csv",
    "data/context/payments-readme.md",
    "data/context/payments.csv",
    "data/context/merchant_category_codes.csv",
    "data/context/fees.json",
    "data/context/merchant_data.json",
    "data/context/manual.md",
]
DATA_DIR = "/tmp/DABstep-data"
for filename in CONTEXT_FILENAMES:
    hf_hub_download(
        repo_id="adyen/DABstep",
        repo_type="dataset",
        filename=filename,
        local_dir=DATA_DIR,
        force_download=True,
    )

CONTEXT_FILENAMES = [f"{DATA_DIR}/{filename}" for filename in CONTEXT_FILENAMES]

for file in CONTEXT_FILENAMES:
    if os.path.exists(file):
        print(f"{file} exists.")
    else:
        print(f"{file} does not exist.")

"""## Agent

Here we will setup a simple zero-shot prompt for the agent. It has two parts, the general prompt and then a quick outline of which files are available.
"""

MODEL_ID = "Qwen/Qwen2.5-Coder-32B-Instruct"
HfApiModel(MODEL_ID)

"""## 3.4 Testing Agent"""

MODEL_ID = "Qwen/Qwen2.5-Coder-32B-Instruct"
MAX_STEPS = 7

agent = CodeAgent(
    tools=[],
    model=HfApiModel(MODEL_ID),
    additional_authorized_imports=[
        "numpy",
        "pandas",
        "json",
        "csv",
        "os",
        "glob",
        "markdown",
    ],
    max_steps=MAX_STEPS,
    verbosity_level=3,
)
# give agent power to open files
agent.python_executor.static_tools.update({"open": open})

PROMPT = """You are an expert data analyst and you will answer factoid questions by loading and referencing the files/documents listed below.
You have these files available:
{context_files}
Don't forget to reference any documentation in the data dir before answering a question.

Here is the question you need to answer:
{question}

Here are the guidelines you must follow when answering the question above:
{guidelines}
"""
question = "What are the unique set of merchants in the payments data?"
guidelines = "Answer with a comma separated list"

PROMPT = PROMPT.format(
    context_files=CONTEXT_FILENAMES, question=question, guidelines=guidelines
)

answer = agent.run(PROMPT)


# You can inspect the steps taken by the agent by doing this
def clean_reasoning_trace(trace: list) -> list:
    for step in trace:
        # Remove memory from logs to make them more compact.
        if hasattr(step, "memory"):
            step.memory = None
        if isinstance(step, ActionStep):
            step.agent_memory = None
    return trace


for step in clean_reasoning_trace(agent.logs):
    print(step)

"""## 3.5 Warmup Agent

We encourage to first get a good performing agent on this smaller dev split of the benchmark since running against the full benchmark can be very costly both in time and compute.

Once we are confident with our agent, we can try to create a submission to the leaderboard using the complete dataset of tasks
"""


def run_benchmark(dataset: datasets.Dataset, agent: CodeAgent) -> list[dict]:
    agent_answers = []
    for task in dataset:
        tid = task["task_id"]

        prompt = PROMPT.format(
            context_files=CONTEXT_FILENAMES,
            question=task["question"],
            guidelines=task["guidelines"],
        )

        answer = agent.run(prompt)

        task_answer = {
            "task_id": str(tid),
            "agent_answer": str(answer),
            "reasoning_trace": str(clean_reasoning_trace(agent.logs)),
        }

        agent_answers.append(task_answer)

    return agent_answers


MAX_STEPS = 7
MODEL_ID = "Qwen/Qwen2.5-Coder-32B-Instruct"

agent = CodeAgent(
    tools=[],  # evaluating zero-shot capabilities of the model when writing code
    model=HfApiModel(MODEL_ID),
    additional_authorized_imports=[
        "numpy",
        "pandas",
        "json",
        "csv",
        "os",
        "glob",
        "markdown",
    ],
    max_steps=MAX_STEPS,
)

# give agent power to open files
agent.python_executor.static_tools.update({"open": open})

# Commented out IPython magic to ensure Python compatibility.
PROMPT = """You are an expert data analyst and you will answer factoid questions by loading and referencing the files/documents listed below.
You have these files available:
{context_files}
Don't forget to reference any documentation in the data dir before answering a question.

Here is the question you need to answer:
{question}

Here are the guidelines you must follow when answering the question above:
{guidelines}
"""

SPLIT = "dev"
MAX_TASKS = 3  # It takes ~20 min to run the agent on all the dev set, so lets start just with the 2 first tasks
RUN_ID = int(time.time())


# load dataset from Hub
dev_task_dataset = datasets.load_dataset(
    "adyen/DABstep", name="tasks", split=f"{SPLIT}[:{MAX_TASKS}]"
)
# %time agent_answers = run_benchmark(dataset=dev_task_dataset, agent=agent)

"""### Evaluation"""

# Lets visualize the answers from our agent
pd.DataFrame(agent_answers)

# Now we evaluate the answers
agent_answers = pd.DataFrame(agent_answers)
tasks_df = dev_task_dataset.to_pandas()
task_scores = evaluate(agent_answers=agent_answers, tasks_with_gt=tasks_df)

# Inspect scores
task_scores = pd.DataFrame(task_scores)
task_scores["correct_answer"] = tasks_df["answer"]
task_scores["question"] = tasks_df["question"]
task_scores

"""## Final submission
Now we run the agent against the full benchmark and create a submission file to submit to the leaderboard

âš ï¸ *THIS WILL TAKE A LONG TIME IF YOU RUN THE FOR THE FULL BENCHMARK (450 tasks)* âš ï¸
"""

from pathlib import Path

# Crate dir to store agent run results
RUNS_DIR = Path().resolve() / "runs"
RUNS_DIR.mkdir(parents=True, exist_ok=True)


def write_jsonl(data: list[dict], filepath: Path) -> None:
    """Write a list of dictionaries to a JSONL file."""
    # Ensure the directory exists
    filepath.parent.mkdir(parents=True, exist_ok=True)

    with open(filepath, "w") as file:
        for entry in data:
            file.write(json.dumps(entry) + "\n")


# Commented out IPython magic to ensure Python compatibility.
PROMPT = """You are an expert data analyst and you will answer factoid questions by loading and referencing the files/documents listed below.
You have these files available:
{context_files}

Here is the question you need to answer:
{question}

Here are the guidelines you must follow when answering the question above:
{guidelines}
"""

# now we run all dataset
SPLIT = "default"
RUN_ID = int(time.time())


# load dataset from Hub
benchmark_task_dataset = datasets.load_dataset(
    "adyen/DABstep", name="tasks", split=SPLIT
)
# %time agent_answers = run_benchmark(dataset=benchmark_task_dataset, agent=agent)

write_jsonl(agent_answers, RUNS_DIR / f"{RUN_ID}.jsonl")

# Lets visualize the answers from our agent
pd.DataFrame(agent_answers)

"""## Submission to Leaderboard

Your submission file should be saved in the `runs` folder of this directory.

Now is the fun part, were you submit it to the Leaderboard and see your score!

The leaderboard is here: https://huggingface.co/spaces/adyen/DABstep

### Analysis your submission scores

Once you have made a submission you can inspect the scores of your submission like these. Task scores are more granular task-level scores.
"""

submissions_dataset = datasets.load_dataset(
    "adyen/DABstep", name="submissions", split="default"
)
task_scores_dataset = datasets.load_dataset(
    "adyen/DABstep", name="task_scores", split="default"
)

AGENT_NAME = (
    "Claude 3.5 Sonnet ReACT Baseline"  # the agent name given in the submission form
)
ORGANISATION = "Adyen"  # the organization name given in the submission form
SUBMISSION_ID = f"{ORGANISATION}-{AGENT_NAME}"

submissions_dataset_df = submissions_dataset.filter(
    lambda row: row["submission_id"] == SUBMISSION_ID
).to_pandas()
task_scores_dataset_df = task_scores_dataset.filter(
    lambda row: row["submission_id"] == SUBMISSION_ID
).to_pandas()

submissions_dataset_df

task_scores_dataset_df

"""## Next Steps

This is a few-shot ReAct prompt by deafult

Some things you can try next:

* do an error analysis and see why you failed certain questions
* tweaking the zero-shot prompt
* try a few-shot prompt
* try different agentic workflows like: CoT
"""
